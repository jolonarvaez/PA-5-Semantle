{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i3m9JjeM5U5"
   },
   "source": [
    "# **Programming Assessment \\#5**\n",
    "\n",
    "Names: Jose Wilfredo S. Narvaez, Manolo L. Enriquez\n",
    "\n",
    "More information on the assessment is found in our Canvas course. Link: https://dlsu.instructure.com/courses/93383/assignments/841058/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxtmCAZwNoeU"
   },
   "source": [
    "# **Load Pre-trained Embeddings**\n",
    "\n",
    "*While you don't have to separate your code into blocks, it might be easier if you separated loading / downloading your data from the main part of your solution. Consider placing all loading of data into the code block below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbvxU2oTM4IV",
    "outputId": "d65d3f76-8c65-4bc5-e459-385421162d90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
    "!unzip wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_nA_c2pJaQgu",
    "outputId": "0b0c2a7b-37ea-45a5-bf48-2a0417f81c67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\jolon\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\jolon\\anaconda3\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\jolon\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\jolon\\anaconda3\\lib\\site-packages (from gensim) (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\jolon\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CtpXZDbUbllT"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wiki-news-300d-1M.vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-1a2a53c29f1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wiki-news-300d-1M.vec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1722\u001b[0m         \"\"\"\n\u001b[1;32m-> 1723\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1724\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1725\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2051\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2052\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2053\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2054\u001b[0m             \u001b[1;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m     fobj = _shortcut_open(\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wiki-news-300d-1M.vec'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "m = gensim.models.KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqKjpUrkOSnC",
    "outputId": "f3c5316c-d2c2-4aa6-af6c-66d69dcb606e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "glove_file = 'glove.6B.50d.txt'\n",
    "glove = {}\n",
    "\n",
    "with open(glove_file) as f:\n",
    "  for line in f:\n",
    "    word, coefs = line.split(maxsplit=1)\n",
    "    glove[word] = np.fromstring(coefs, 'f', sep=' ')\n",
    "\n",
    "print(f'Loaded {len(glove)} word vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8YCZLi-N0uR"
   },
   "source": [
    "# **Your Implementation**\n",
    "\n",
    "*Again, you don't have to have everything in one block. Use the notebook according to your preferences with the goal of fulfilling the assessment in mind.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXTUITQejhMI"
   },
   "source": [
    "### Glove Word Vectors\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVV8ewQrS5-s"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(coefs1, coefs2):\n",
    "  return np.dot(coefs1, coefs2) / (norm(coefs1) * norm(coefs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iAlKU8v8TTyQ",
    "outputId": "dc7a9de0-e6d6-47ff-9a9f-c514804ce695"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rand_word, rand_coefs = random.choice(list(glove.items()))\n",
    "\n",
    "print(f'Randomly selected word: {rand_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6nz336vsToPs",
    "outputId": "e8545643-6ceb-40bf-cc67-e8ba5938640a"
   },
   "outputs": [],
   "source": [
    "similarities = {}\n",
    "\n",
    "for word, coefs in glove.items():\n",
    "  similarities[word] = cosine_similarity(glove[word], rand_coefs)\n",
    "\n",
    "similarities\n",
    "\n",
    "result = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('Closest...')\n",
    "\n",
    "k, v = result[10]\n",
    "print(f'10th - {k} {v}')\n",
    "\n",
    "k, v = result[100]\n",
    "print(f'100th - {k} {v}')\n",
    "\n",
    "k, v = result[1000]\n",
    "print(f'1000th - {k} {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-hnwnh0nD2x",
    "outputId": "c8c8796e-e7cf-45a7-f147-ccc7e1cdfbe8"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "  try:\n",
    "    guess = input('Your guess: ')\n",
    "    similarity = cosine_similarity(glove[guess], rand_coefs)\n",
    "    print(f'{guess}: {similarity}')\n",
    "  except:\n",
    "    print('Word not found')\n",
    "  \n",
    "  if guess == rand_word:\n",
    "    print('Great job on guessing the word!')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5bURP6SijqW"
   },
   "source": [
    "### English Word Vectors\n",
    "https://fasttext.cc/docs/en/english-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9Esrq3tehXC",
    "outputId": "4cc826c1-c789-4048-ce24-9c113a8b95df"
   },
   "outputs": [],
   "source": [
    "rand_word = random.choice(m.index_to_key)\n",
    "\n",
    "print(f'Randomly selected word: {rand_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3zaBKw-fMn1",
    "outputId": "cb16ca4c-3967-4852-e20b-e193f1224ef7"
   },
   "outputs": [],
   "source": [
    "result = m.most_similar(positive=[rand_word], topn=1000)\n",
    "\n",
    "print('Closest...')\n",
    "\n",
    "k, v = result[9]\n",
    "print(f'10th - {k} {v}')\n",
    "\n",
    "k, v = result[99]\n",
    "print(f'100th - {k} {v}')\n",
    "\n",
    "k, v = result[999]\n",
    "print(f'1000th - {k} {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8vLjTyHg2y7",
    "outputId": "9f502591-0b16-49d0-cc2d-93176c126144"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "  try:\n",
    "    guess = input('Your guess: ')\n",
    "    similarity = m.similarity(rand_word, guess)\n",
    "    print(f'{guess}: {similarity}')\n",
    "  except:\n",
    "    print('Word not found')\n",
    "  \n",
    "  if guess == rand_word:\n",
    "    print('Great job on guessing the word!')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3smvUR6OXUa"
   },
   "source": [
    "# **Your Relfection / Takeaway / Analysis**\n",
    "\n",
    "*Kindly place the rest of your write up. More information is found in the Canvas write up.*\n",
    "\n",
    "Our approach to the objective is pretty straight forward. After loading the embeddings, we used python's random function to randomly select a word from the word vector dictionary. For this instance, the embedding used is the Glove word vectors. Then create a similarities dictionary with each word making the key of each dictionary entry the word itself and its value as the value it gets after computing for its cosine similarity which is defined in another function. This data structure just helps the implementation retrieve values faster since we are dealing with a large data set. Additionally, we did not modify the pre-trained embeddings. After this, we sort the similarity dictionary to get the ranking of each word, in relation to how similar a word is to the actual given word. Inorder to simulate the semantle game, a while loop was used to keep accepting input and to keep displaying their respective similarity score from the dictionary until the user correctly guesses the word.\n",
    "\n",
    "While experimenting with the Glove Word Vectors and the English Word Vectors, the similatiry scores captured were similar to each other in terms of semantic similarity. With this, no matter what dimenstions you have for your embeddings, it really boils down to your computer's processing power to manipulate the data.  \n",
    "\n",
    "In conclusion, it was pretty interesting how these words could be assigned a meaning through a number. It was also a pointing interest on how computers could understand meaning under its own language which may seem absurd towards humans. Additionally, compared to semantle, the only main difference between the online game and our implementation is how the score is displayed. Ours goes from 0 to 1 while Semantle goes from 0 to 100 which is easily adjustable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PA5_Replicating_Semantle.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
